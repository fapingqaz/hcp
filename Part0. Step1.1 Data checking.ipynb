{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fd988a",
   "metadata": {},
   "source": [
    "# 划分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d019a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from onekey_algo.custom.components.comp2 import split_dataset4sol\n",
    "from onekey_algo import get_param_in_cwd\n",
    "from onekey_algo.custom.utils import print_join_info\n",
    "\n",
    "clinic = pd.read_csv(r'E:/20240601-BiQiu/label.csv')\n",
    "data = clinic[['ID', 'label', 'group']]\n",
    "train_data = data[data['group'] == 'train']\n",
    "test_data = data[data['group'] != 'train']\n",
    "\n",
    "rt = split_dataset4sol(train_data, train_data['label'], cv=False, n_trails=10, test_size=0.3, save_dir='.', shuffle=True)\n",
    "for idx, (train, val) in enumerate(rt):\n",
    "    val['group'] = 'val'\n",
    "    rnd = pd.concat([train, val, test_data], axis=0)\n",
    "    display(rnd['group'].value_counts())\n",
    "    rnd.to_csv(f'split_info/label-RND-{idx}.csv', index=False)\n",
    "#     pd.merge(rnd[rnd['group'] == 'train'], samples.drop_duplicates('ID'), on='ID', how='inner')[['ID', 'label']].to_csv(f'split_info/train3d-RND-{idx}.txt', \n",
    "#                                                                                                   sep='\\t', header=None, index=None)\n",
    "#     pd.merge(rnd[rnd['group'] != 'train'], samples.drop_duplicates('ID'), on='ID', how='inner')[['ID', 'label']].to_csv(f'split_info/val3d-RND-{idx}.txt',\n",
    "#                                                                                                   sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0327059",
   "metadata": {},
   "source": [
    "# 拆分病理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from onekey_algo.custom.components.comp2 import split_dataset4sol\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "root = r'E:/20240601-BiQiu/Pathomics'\n",
    "patches = pd.DataFrame(glob(os.path.join(root, 'patches_norm', '*', '*.jpg')), columns=['fpath'])\n",
    "patches['ID'] = patches['fpath'].map(lambda x: os.path.basename(os.path.dirname(x)))\n",
    "patches['filename'] = patches['fpath'].map(lambda x: os.path.basename(x))\n",
    "patches                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nact = pd.read_csv(r'E:/20240601-BiQiu/NACT.csv')\n",
    "nact = nact[nact['NACT'] == 0]\n",
    "for idx in range(10):\n",
    "    l = pd.merge(pd.read_csv(f'split_info/label-RND-{idx}.csv'), nact[['ID']], on='ID', how='inner')\n",
    "    l['ID'] = l['ID'].map(lambda x: x.replace('.nii.gz', ''))\n",
    "    patch_info = pd.merge(patches, l, on='ID', how='inner')\n",
    "    os.makedirs(os.path.join(root, 'split_info'), exist_ok=True)\n",
    "    train = patch_info[patch_info['group'] == 'train'][['fpath', 'ID', 'label']]\n",
    "    train[['fpath', 'label']].to_csv(os.path.join(root, 'split_info', f'train-RND-{idx}.txt'),  index=False, header=False, sep='\\t')\n",
    "    valtest = patch_info[patch_info['group'] != 'train'][['fpath', 'ID', 'label']]\n",
    "    valtest[['fpath', 'label']].to_csv(os.path.join(root, 'split_info', f'val-RND-{idx}.txt'), index=False, header=False, sep='\\t')    \n",
    "    print(f\"随机划分：{idx}，训练集：{train.shape[0]}, {len(np.unique(train['ID']))}, 测试集集：{valtest.shape[0]}, {len(np.unique(valtest['ID']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "set(pd.read_csv(r'E:/20240601-BiQiu/label.csv')['ID']) - set(pd.read_csv(r'group.csv')['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a619538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log = pd.concat([pd.read_csv(f'results/Path_LR_{s}.csv') for s in ['train', 'val', 'test']], axis=0)\n",
    "display(log)\n",
    "set(pd.read_csv(r'features/path1_pred_histogram.csv')['ID']) - set(log['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c806faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log = pd.concat([pd.read_csv(f'results/Habitat_LR_{s}.csv') for s in ['train', 'val', 'test']], axis=0)\n",
    "display(log)\n",
    "aid = pd.merge(pd.read_csv('E:/20240601-BiQiu/label.csv'), pd.DataFrame(os.listdir(r'E:/20240601-BiQiu/Radiomics/ADC/'), columns=['ID']))\n",
    "set(aid['ID']) - set(log['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nact = pd.read_csv(r'E:/20240601-BiQiu/NACT.csv')\n",
    "nact = nact[nact['NACT'] == 0]\n",
    "\n",
    "set(nact['ID']) - set(pd.read_csv('data/clinic_sel.csv')['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e6624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
