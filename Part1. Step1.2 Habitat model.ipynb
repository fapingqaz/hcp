{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dac6cf",
   "metadata": {},
   "source": [
    "# Comp1-传统组学\n",
    "\n",
    "主要适配于传统组学的建模和刻画。典型的应用场景探究rad_score最最终临床诊断的作用。\n",
    "\n",
    "数据的一般形式为(具体文件,文件夹名可以不同)：\n",
    "1. `images`文件夹，存放研究对象所有的CT、MRI等数据。\n",
    "2. `masks`文件夹, 存放手工（Manuelly）勾画的ROI区域。与images文件夹的文件意义对应。\n",
    "3. `label.txt`文件，每个患者对应的标签，例如肿瘤的良恶性、5年存活状态等。\n",
    "\n",
    "## Onekey步骤\n",
    "\n",
    "1. 数据校验，检查数据格式是否正确。\n",
    "2. 组学特征提取，如果第一步检查数据通过，则提取对应数据的特征。\n",
    "3. 读取标注数据信息。\n",
    "4. 特征与标注数据拼接。形成数据集。\n",
    "5. 查看一些统计信息，检查数据时候存在异常点。\n",
    "6. 正则化，将数据变化到服从 N~(0, 1)。\n",
    "7. 通过相关系数，例如spearman、person等筛选出特征。\n",
    "8. 构建训练集和测试集，这里使用的是随机划分，正常多中心验证，需要大家根据自己的场景构建两份数据。\n",
    "9. 通过Lasso筛选特征，选取其中的非0项作为后续模型的特征。\n",
    "10. 使用机器学习算法，例如LR、SVM、RF等进行任务学习。\n",
    "11. 模型结果可视化，例如AUC、ROC曲线，混淆矩阵等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获得视频教程\n",
    "import os\n",
    "os.environ['DISABLE_VIDEO'] = 'TRUE'\n",
    "from onekey_algo.custom.Manager import onekey_show\n",
    "onekey_show('传统组学任务')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556a1cd",
   "metadata": {},
   "source": [
    "## 一、数据校验\n",
    "首先需要检查诊断数据，如果显示`检查通过！`择可以正常运行之后的，否则请根据提示调整数据。\n",
    "\n",
    "**注意**：这里要求images和masks文件夹中的文件名必须一一对应。e.g. `1.nii.gz`为images中的一个文件，在masks文件夹必须也存在一个`1.nii.gz`文件。\n",
    "\n",
    "当然也可以使用自定义的函数，获取解析数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f195b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据检验视频\n",
    "onekey_show('传统组学任务|数据检验')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f47352",
   "metadata": {},
   "source": [
    "### 指定数据\n",
    "\n",
    "此模块有3个需要自己定义的参数\n",
    "\n",
    "1. `mydir`: 数据存放的路径。\n",
    "2. `labelf`: 每个样本的标注信息文件。\n",
    "3. `labels`: 要让AI系统学习的目标，例如肿瘤的良恶性、T-stage等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c779d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('features', exist_ok=True)\n",
    "\n",
    "# 设置任务Task前缀\n",
    "task_type = 'Habitat_'\n",
    "# 设置数据目录\n",
    "# mydir = r'你自己数据的路径'\n",
    "mydir = get_param_in_cwd('radio_dir') or okds.ct\n",
    "if mydir == okds.ct:\n",
    "    print(f'正在使用Onekey数据：{okds.ct}，如果不符合预期，请修改目录位置！')\n",
    "# 对应的标签文件\n",
    "group_info = get_param_in_cwd('dataset_column') or 'group'\n",
    "labelf = get_param_in_cwd('label_file') or os.path.join(mydir, 'label.csv')\n",
    "# 读取标签数据列名\n",
    "labels = [get_param_in_cwd('task_column') or 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2a7b3",
   "metadata": {},
   "source": [
    "### images和masks匹配\n",
    "\n",
    "这里要求images和masks文件夹中的文件名必须一一对应。e.g. `1.nii.gz`为images中的一个文件，在masks文件夹必须也存在一个`1.nii.gz`文件。\n",
    "\n",
    "当然也可以使用自定义的函数，获取解析数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from onekey_algo.custom.components.Radiology import diagnose_3d_image_mask_settings, get_image_mask_from_dir\n",
    "\n",
    "# # 生成images和masks对，一对一的关系。也可以自定义替换。\n",
    "# for modal in get_param_in_cwd('modals'):\n",
    "#     images, masks = get_image_mask_from_dir(mydir, images=f'{modal}', masks=f'habitat\\cluster_4')\n",
    "# #     diagnose_3d_image_mask_settings(images, masks, verbose=True)\n",
    "#     print(f'{modal}, 获取到{len(images)}个样本。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed41d5",
   "metadata": {},
   "source": [
    "# 传统组学特征\n",
    "\n",
    "使用pyradiomics提取传统组学特征，正常这块不需要修改，下面是具体的Onekey封装的接口。\n",
    "\n",
    "```python\n",
    "def extract(self, images: Union[str, List[str]], \n",
    "            masks: Union[str, List[str]], labels: Union[int, List[int]] = 1, settings=None)\n",
    "\"\"\"\n",
    "    * images: List结构，待提取的图像列表。\n",
    "    * masks: List结构，待提取的图像对应的mask，与Images必须一一对应。\n",
    "    * labels: 提取标注为什么标签的特征。默认为提取label=1的。\n",
    "    * settings: 其他提取特征的参数。默认为None。\n",
    "\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "def get_label_data_frame(self, label: int = 1, column_names=None, images='images', masks='labels')\n",
    "\"\"\"\n",
    "    * label: 获取对应label的特征。\n",
    "    * columns_names: 默认为None，使用程序设定的列名即可。\n",
    "\"\"\"\n",
    "```\n",
    "    \n",
    "```python\n",
    "def get_image_mask_from_dir(root, images='images', masks='labels')\n",
    "\"\"\"\n",
    "    * root: 待提取特征的目录。\n",
    "    * images: root目录中原始数据的文件夹名。\n",
    "    * masks: root目录中标注数据的文件夹名。\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93aacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取视频\n",
    "onekey_show('传统组学任务|特征提取')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from onekey_algo.custom.components.Radiology import ConventionalRadiomics\n",
    "from onekey_algo.custom.components.habitat import habitat_feature_fusion\n",
    " \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rad_data = None\n",
    "habitats = [1, 2, 3, 4]\n",
    "param_file = get_param_in_cwd('extractor_settings')\n",
    "for modal in get_param_in_cwd('modals'):\n",
    "    rad_ = []\n",
    "    for habitat in habitats:\n",
    "        if os.path.exists(f'features/h{habitat}_rad_features_{modal}.csv'):\n",
    "            rad_data_ = pd.read_csv(f'features/h{habitat}_rad_features_{modal}.csv', header=0)\n",
    "        else:\n",
    "            images, masks = get_image_mask_from_dir(mydir, images=f'{modal}', masks=f'habitat\\cluster_4')\n",
    "            radiomics = ConventionalRadiomics(param_file, correctMask=True)\n",
    "            radiomics.extract(images, masks, workers=6, labels=habitat)\n",
    "            rad_data_ = radiomics.get_label_data_frame(label=habitat)\n",
    "            rad_data_.columns = [f\"{c.replace('-', '_')}_h{habitat}\" if c != 'ID' else 'ID' for c in rad_data_.columns]\n",
    "            rad_data_.to_csv(f'features/h{habitat}_rad_features_{modal}.csv', header=True, index=False)\n",
    "        rad_.append(rad_data_)\n",
    "    if os.path.exists(f'features/rad_features_habitat_{modal}.csv') and False:\n",
    "        habitat_data = pd.read_csv(f'features/rad_features_habitat_{modal}.csv')\n",
    "    else:\n",
    "        habitat_data = habitat_feature_fusion(*rad_, mode='fill')\n",
    "        habitat_data.to_csv(f'features/rad_features_habitat_{modal}.csv', index=False)\n",
    "    habitat_data.columns = [f\"{c}_{modal}\" if c != 'ID' else 'ID' for c in habitat_data.columns]\n",
    "    if rad_data is None:\n",
    "        rad_data = habitat_data\n",
    "    else:\n",
    "        rad_data = pd.merge(rad_data, habitat_data, on='ID', how='inner')\n",
    "rad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap = pd.read_csv('features/Habitat_after_lasso.csv')\n",
    "# rad_data[ap.columns].to_csv('features/Habitat_after_lasso.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52a592",
   "metadata": {},
   "source": [
    "## 特征统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3413c7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sorted_counts = pd.DataFrame([c.split('_')[-4] for c in rad_data.columns if c !='ID']).value_counts()\n",
    "sorted_counts = pd.DataFrame(sorted_counts, columns=['count']).reset_index()\n",
    "sorted_counts = sorted_counts.sort_values(0)\n",
    "sorted_counts.columns = ['feature_group', 'count',]\n",
    "display(sorted_counts)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot(121)\n",
    "plt.pie(sorted_counts['count'], labels=[i for i in sorted_counts['feature_group']], startangle=0,\n",
    "        counterclock = False, autopct = '%.1f%%')\n",
    "# plt.bar_label(bar.containers[0])\n",
    "ax = plt.subplot(122)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "bar = sns.barplot(data=sorted_counts, x='feature_group', y='count', )\n",
    "plt.savefig(f'img/{task_type}feature_ratio.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715d329",
   "metadata": {},
   "source": [
    "## 标注数据\n",
    "\n",
    "数据以csv格式进行存储，这里如果是其他格式，可以使用自定义函数读取出每个样本的结果。\n",
    "\n",
    "要求label_data为一个`DataFrame`格式，包括ID列以及后续的labels列，可以是多列，支持Multi-Task。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = 'group'\n",
    "label_data = pd.read_csv(labelf)\n",
    "label_data[group_info] = label_data[group_info].map(lambda x: x if x in ['train', 'val'] else 'test')\n",
    "label_data['ID'] = label_data['ID'].map(lambda x: f\"{x}.nii.gz\" if not (f\"{x}\".endswith('.nii.gz') or  f\"{x}\".endswith('.nii')) else x)\n",
    "# g = pd.read_csv('ids.csv')\n",
    "# label_data = pd.merge(label_data, g, on='ID', how='inner')\n",
    "label_data = label_data[['ID', group_info] + labels]\n",
    "label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814b27",
   "metadata": {},
   "source": [
    "## 特征拼接 \n",
    "\n",
    "将标注数据`label_data`与`rad_data`进行合并，得到训练数据。\n",
    "\n",
    "**注意：** \n",
    "1. 需要删掉ID这一列\n",
    "2. 如果发现数据少了，需要自行检查数据是否匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78982ba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 删掉ID这一列。\n",
    "from onekey_algo.custom.utils import print_join_info\n",
    "print_join_info(rad_data, label_data)\n",
    "combined_data = pd.merge(rad_data, label_data, on=['ID'], how='inner')\n",
    "# combined_data[['ID'] + selected_features[0]].to_csv('features/sel_habitat.csv', index=False)\n",
    "ids = combined_data['ID']\n",
    "combined_data = combined_data.drop(['ID'], axis=1)\n",
    "display(combined_data[labels].value_counts())\n",
    "display(combined_data[group_info].value_counts())\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('features/Habitat_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e19ce",
   "metadata": {},
   "source": [
    "## 获取到数据的统计信息\n",
    "\n",
    "1. count，统计样本个数。\n",
    "2. mean、std, 对应特征的均值、方差\n",
    "3. min, 25%, 50%, 75%, max，对应特征的最小值，25,50,75分位数，最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec35ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fc73b",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "`normalize_df` 为onekey中正则化的API，将数据变化到0均值1方差。正则化的方法为\n",
    "\n",
    "$column = \\frac{column - mean}{std}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import normalize_df\n",
    "data = normalize_df(combined_data, not_norm=labels, group=group_info, use_train=True)\n",
    "data = data.dropna(axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1154c13",
   "metadata": {},
   "source": [
    "## 统计检验\n",
    "\n",
    "通过ttest或者utest进行特征筛选。\n",
    "\n",
    "**注意** ：此步骤不是论文的标配，所以用不用在自己的选择，可以通过修改pvalue的值进行调整，默认是0.05为显著。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from onekey_algo.custom.components.stats import clinic_stats\n",
    "\n",
    "sub_data = data[data[group_info] == 'train']\n",
    "stats = clinic_stats(sub_data, stats_columns=list(data.columns[0:-2]), label_column=labels[0], \n",
    "                     continuous_columns=list(data.columns[0:-2]))\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3c4f3",
   "metadata": {},
   "source": [
    "#### 输出特征分布的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5579be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def map2float(x):\n",
    "    try:\n",
    "        return float(str(x)[1:])\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "stats[['pvalue']] = stats[['pvalue']].applymap(map2float)\n",
    "stats[['group']] = stats[['feature_name']].applymap(lambda x: x.split('_')[-4])\n",
    "stats = stats[['feature_name', 'pvalue', 'group']]\n",
    "g = sns.catplot(x=\"group\", y=\"pvalue\", data=stats, kind=\"violin\")\n",
    "g.fig.set_size_inches(15,10)\n",
    "# sns.stripplot(x=\"group\", y=\"pvalue\", data=stats, ax=g.ax, color='black')\n",
    "plt.savefig(f'img/{task_type}feature_stats.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4427ca",
   "metadata": {},
   "source": [
    "#### 调整pvalue进行筛选。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3830694",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = 0.05\n",
    "sel_feature = list(stats[stats['pvalue'] < pvalue]['feature_name']) + labels + [group_info]\n",
    "sub_data = sub_data[sel_feature]\n",
    "pd.concat([ids, data], axis=1).to_csv(f'features/{task_type}after_stats.csv', index=False)\n",
    "sub_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662f002",
   "metadata": {},
   "source": [
    "### 相关系数\n",
    "\n",
    "计算相关系数的方法有3种可供选择\n",
    "1. pearson （皮尔逊相关系数）: standard correlation coefficient\n",
    "\n",
    "2. kendall (肯德尔相关性系数) : Kendall Tau correlation coefficient\n",
    "\n",
    "3. spearman (斯皮尔曼相关性系数): Spearman rank correlation\n",
    "\n",
    "三种相关系数参考：https://blog.csdn.net/zmqsdu9001/article/details/82840332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f86bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关系数视频\n",
    "onekey_show('传统组学任务|相关系数')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = sub_data[[c for c in sub_data.columns if c not in labels]].corr('pearson')\n",
    "# kendall_corr = data[[c for c in data.columns if c not in labels]].corr('kendall')\n",
    "# spearman_corr = data[[c for c in data.columns if c not in labels]].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46fc3c",
   "metadata": {},
   "source": [
    "### 相关系数可视化\n",
    "\n",
    "通过修改变量名，可以可视化不同相关系数下的相关矩阵。\n",
    "\n",
    "**注意**：当特征特别多的时候（大于100），尽量不要可视化，否则运行时间会特别长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from onekey_algo.custom.components.comp1 import draw_matrix\n",
    "\n",
    "if data.shape[1] < 100:\n",
    "    plt.figure(figsize=(50.0, 40.0))\n",
    "\n",
    "    # 选择可视化的相关系数\n",
    "    draw_matrix(pearson_corr, annot=True, cmap='YlGnBu', cbar=False)\n",
    "    plt.savefig(f'img/{task_type}feature_corr.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82133527",
   "metadata": {},
   "source": [
    "### 聚类分析\n",
    "\n",
    "通过修改变量名，可以可视化不同相关系数下的相聚类分析矩阵。\n",
    "\n",
    "注意：当特征特别多的时候（大于100），尽量不要可视化，否则运行时间会特别长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24919e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if data.shape[1] < 100:\n",
    "    pp = sns.clustermap(pearson_corr, linewidths=.5, figsize=(50.0, 40.0), cmap='YlGnBu')\n",
    "    plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "    plt.savefig(f'img/{task_type}feature_cluster.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e78fdf",
   "metadata": {},
   "source": [
    "### 特征筛选 -- 相关系数\n",
    "\n",
    "根据相关系数，对于相关性比较高的特征（一般文献取corr>0.9），两者保留其一。\n",
    "\n",
    "```python\n",
    "def select_feature(corr, threshold: float = 0.9, keep: int = 1, topn=10, verbose=False):\n",
    "    \"\"\"\n",
    "    * corr, 相关系数矩阵。\n",
    "    * threshold，筛选的相关系数的阈值，大于阈值的两者保留其一（可以根据keep修改，可以是其二...）。默认阈值为0.9\n",
    "    * keep，可以选择大于相关系数，保留几个，默认只保留一个。\n",
    "    * topn, 每次去掉多少重复特征。\n",
    "    * verbose，是否打印日志\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35748f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征筛选视频\n",
    "onekey_show('传统组学任务|特征筛选')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e96efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from onekey_algo.custom.components.comp1 import select_feature, select_feature_mrmr\n",
    "\n",
    "sel_feature = select_feature(pearson_corr, threshold=0.9, topn=256, verbose=False)\n",
    "sel_feature = select_feature_mrmr(sub_data[sel_feature + labels], num_features=64)\n",
    "sel_feature += labels + [group_info]\n",
    "sel_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bc71a",
   "metadata": {},
   "source": [
    "### 过滤特征\n",
    "\n",
    "通过`sel_feature`过滤出筛选出来的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1309a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = data[sel_feature]\n",
    "pd.concat([ids, combined_data[sel_feature]], axis=1).to_csv(f'features/{task_type}after_pearson.csv', index=False)\n",
    "sel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0768e5b",
   "metadata": {},
   "source": [
    "### 样本可视化\n",
    "\n",
    "根据特征和label信息，将rad features降维到2维，看不同的label样本在二维空间的分布。\n",
    "\n",
    "**注意**：由于特征空间维度极高，降维难免会有损失，所以二维的可视化仅供参考。\n",
    "\n",
    "目前支持的:\n",
    "\n",
    "| **降维方法** | **Method名称**                                                 |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| LLE      | Standard LLE, Modified LLE                                   |\n",
    "| PCA      | t-SNE, NCA                                                      |\n",
    "| SVD      | Truncated SVD                                              |\n",
    "| Model Based      | Random projection, Isomap, MDS, Random Trees,Spectral       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import analysis_features\n",
    "analysis_features(data[sel_feature[:-2]], data[labels[0]], methods='t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4be710",
   "metadata": {},
   "source": [
    "## 构建数据\n",
    "\n",
    "将样本的训练数据X与监督信息y分离出来，并且对训练数据进行划分，一般的划分原则为80%-20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8058ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onekey_algo.custom.components as okcomp\n",
    "from collections import OrderedDict\n",
    "n_classes = 2\n",
    "train_data = sel_data[(sel_data[group_info] == 'train')]\n",
    "train_ids = ids[train_data.index]\n",
    "train_data = train_data.reset_index()\n",
    "train_data = train_data.drop('index', axis=1)\n",
    "y_data = train_data[labels]\n",
    "X_data = train_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "subsets = [s for s in get_param_in_cwd('subsets') if s != 'train']\n",
    "val_datasets = OrderedDict()\n",
    "for subset in subsets:\n",
    "    val_data = sel_data[sel_data[group_info] == subset]\n",
    "    val_ids = ids[val_data.index]\n",
    "    val_data = val_data.reset_index()\n",
    "    val_data = val_data.drop('index', axis=1)\n",
    "    y_val_data = val_data[labels]\n",
    "    X_val_data = val_data.drop(labels + [group_info], axis=1)\n",
    "    val_datasets[subset] = [X_val_data, y_val_data, val_ids]\n",
    "\n",
    "y_all_data = sel_data[labels]\n",
    "X_all_data = sel_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "column_names = X_data.columns\n",
    "print(f\"训练集样本数：{X_data.shape}，\", '，'.join([f\"{subset}样本数：{d_[0].shape}\" for subset, d_ in val_datasets.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14751e1b",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "初始化Lasso模型，alpha为惩罚系数。具体的参数文档可以参考：[文档](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso)\n",
    "\n",
    "### 交叉验证\n",
    "\n",
    "不同Lambda下的，特征的的权重大小。\n",
    "```python\n",
    "def lasso_cv_coefs(X_data, y_data, points=50, column_names: List[str] = None, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        X_data: 训练数据\n",
    "        y_data: 监督数据\n",
    "        points: 打印多少个点。默认50\n",
    "        column_names: 列名，默认为None，当选择的数据很多的时候，建议不要添加此参数\n",
    "        **kwargs: 其他用于打印控制的参数。\n",
    "\n",
    "    \"\"\"\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33550fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None)\n",
    "plt.savefig(f'img/{task_type}feature_lasso.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ecc7c",
   "metadata": {},
   "source": [
    "### 模型效能\n",
    "\n",
    "```python\n",
    "def lasso_cv_efficiency(X_data, y_data, points=50, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        Xdata: 训练数据\n",
    "        ydata: 测试数据\n",
    "        points: 打印的数据密度\n",
    "        **kwargs: 其他的图像样式\n",
    "            # 数据点标记, fmt=\"o\"\n",
    "            # 数据点大小, ms=3\n",
    "            # 数据点颜色, mfc=\"r\"\n",
    "            # 数据点边缘颜色, mec=\"r\"\n",
    "            # 误差棒颜色, ecolor=\"b\"\n",
    "            # 误差棒线宽, elinewidth=2\n",
    "            # 误差棒边界线长度, capsize=2\n",
    "            # 误差棒边界厚度, capthick=1\n",
    "    Returns:\n",
    "    \"\"\"\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e95575",
   "metadata": {},
   "outputs": [],
   "source": [
    "okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50)\n",
    "plt.savefig(f'img/{task_type}feature_mse.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfedef",
   "metadata": {},
   "source": [
    "### 惩罚系数\n",
    "\n",
    "使用交叉验证的惩罚系数作为模型训练的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "models = []\n",
    "for label in labels:\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(X_data, y_data[label])\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acebd04f",
   "metadata": {},
   "source": [
    "### 特征筛选\n",
    "\n",
    "筛选出其中coef > 0的特征。并且打印出相应的公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEF_THRESHOLD = 1e-6 # 筛选的特征阈值\n",
    "scores = []\n",
    "selected_features = []\n",
    "for label, model in zip(labels, models):\n",
    "    feat_coef = [(feat_name, coef) for feat_name, coef in zip(column_names, model.coef_) \n",
    "                 if COEF_THRESHOLD is None or abs(coef) > COEF_THRESHOLD]\n",
    "    selected_features.append([feat for feat, _ in feat_coef])\n",
    "    formula = ' '.join([f\"{coef:+.6f} * {feat_name}\" for feat_name, coef in feat_coef])\n",
    "    score = f\"{label} = {model.intercept_} {'+' if formula[0] != '-' else ''} {formula}\"\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b2310",
   "metadata": {},
   "source": [
    "### 特征权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_coef = sorted(feat_coef, key=lambda x: x[1])\n",
    "feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])\n",
    "feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh')\n",
    "\n",
    "plt.savefig(f'img/{task_type}feature_weights.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f48dae",
   "metadata": {},
   "source": [
    "### 进一步筛选特征\n",
    "\n",
    "使用Lasso筛选出来的Coefficients比较高的特征作为训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585adbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data[selected_features[0]]\n",
    "for subset in val_datasets:\n",
    "    val_datasets[subset][0] = val_datasets[subset][0][selected_features[0]]\n",
    "pd.concat([ids, combined_data[selected_features[0]]], axis=1).to_csv(f'features/{task_type}after_lasso.csv', index=False)\n",
    "X_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360da2df",
   "metadata": {},
   "source": [
    "## 模型筛选\n",
    "\n",
    "根据筛选出来的数据，做模型的初步选择。当前主要使用到的是Onekey中的\n",
    "\n",
    "1. SVM，支持向量机，引用参考。\n",
    "2. KNN，K紧邻，引用参考。\n",
    "3. Decision Tree，决策树，引用参考。\n",
    "4. Random Forests, 随机森林，引用参考。\n",
    "5. XGBoost, bosting方法。引用参考。\n",
    "6. LightGBM, bosting方法，引用参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_names = get_param_in_cwd('ml_models')\n",
    "models = okcomp.comp1.create_clf_model(model_names)\n",
    "# models['LR'] = LogisticRegression(penalty='none', max_iter=100)\n",
    "# models['RandomForest'] = RandomForestClassifier(n_estimators=5, max_depth=4, min_samples_split=2, random_state=0)\n",
    "# models['XGBoost'] = XGBClassifier(n_estimators=4, objective='binary:logistic', max_depth=3, min_child_weight=1.2,\n",
    "#                                           use_label_encoder=False, eval_metric='error')\n",
    "# models['LightGBM'] = LGBMClassifier(n_estimators=6,  max_depth=3, min_child_weight=0.5,)\n",
    "# models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=30, max_depth=3, min_samples_split=2, random_state=0)\n",
    "    \n",
    "model_names = list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f6ada",
   "metadata": {},
   "source": [
    "### 交叉验证\n",
    "\n",
    "`n_trails`指定随机次数，每次采用的是80%训练，随机20%进行测试，找到最好的模型，以及对应的最好的数据划分。\n",
    "\n",
    "\n",
    "```python\n",
    "def get_bst_split(X_data: pd.DataFrame, y_data: pd.DataFrame,\n",
    "            models: dict, test_size=0.2, metric_fn=accuracy_score, n_trails=10,\n",
    "            cv: bool = False, shuffle: bool = False, metric_cut_off: float = None, random_state=None):\n",
    "    \"\"\"\n",
    "    寻找数据集中最好的数据划分。\n",
    "    Args:\n",
    "        X_data: 训练数据\n",
    "        y_data: 监督数据\n",
    "        models: 模型名称，Dict类型、\n",
    "        test_size: 测试集比例，只有当cv=False时生效\n",
    "        metric_fn: 评价模型好坏的函数，默认准确率，可选roc_auc_score。\n",
    "        n_trails: 尝试多少次寻找最佳数据集划分。\n",
    "        cv: 是否是交叉验证，默认是False，当为True时，n_trails为交叉验证的n_fold\n",
    "        shuffle: 是否进行随机打乱\n",
    "        metric_cut_off: 当metric_fn的值达到多少时进行截断。\n",
    "        random_state: 随机种子\n",
    "\n",
    "    Returns: {'max_idx': max_idx, \"max_model\": max_model, \"max_metric\": max_metric, \"results\": results}\n",
    "\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**注意：这里采用了【挑数据】，如果想要严谨，请修改`n_trails=1`。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f86f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 随机使用n_trails次数据划分，找到最好的一次划分方法，并且保存在results中。\n",
    "results = okcomp.comp1.get_bst_split(X_data, y_data, models, test_size=0.2, metric_fn=roc_auc_score, n_trails=5, cv=False, \n",
    "                                     random_state=75)\n",
    "# _, (X_train_sel, X_test_sel, y_train_sel, y_test_sel) = results['results'][results['max_idx']]\n",
    "trails, _ = zip(*results['results'])\n",
    "cv_results = pd.DataFrame(trails, columns=model_names)\n",
    "# 可视化每个模型在不同的数据划分中的效果。\n",
    "sns.barplot(data=cv_results)\n",
    "plt.ylabel('AUC %')\n",
    "plt.xlabel('Model Nmae')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.ylim(0.5,)\n",
    "plt.savefig(f'img/{task_type}model_cv.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ff1e1",
   "metadata": {},
   "source": [
    "## 模型筛选\n",
    "\n",
    "使用最好的数据划分，进行后续的模型研究。\n",
    "\n",
    "**注意**: 一般情况下论文使用的是随机划分的数据，但也有些论文使用【刻意】筛选的数据划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# from onekey_algo.custom.components.comp1 import plot_feature_importance, smote_resample\n",
    "# from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from onekey_algo.custom.components.delong import calc_95_CI\n",
    "# from onekey_algo.custom.components.metrics import analysis_pred_binary\n",
    "\n",
    "\n",
    "# os.makedirs('models', exist_ok=True)\n",
    "# sel_model = 'RandomForest'\n",
    "# maxi = 0\n",
    "# for p in range(1, 100):\n",
    "#     for md in range(1, 5):\n",
    "#         targets = []\n",
    "#         print(p, md)\n",
    "#         for l in labels:\n",
    "#             new_models = okcomp.comp1.create_clf_model([sel_model])\n",
    "#             if sel_model == 'LR':\n",
    "#                 new_models['LR'] = LogisticRegression(penalty='l2', max_iter=p, C=md/5)\n",
    "#         #     new_models['SVM'] = SVC(probability=True, kernel='rbf')\n",
    "#             if sel_model == 'RandomForest':\n",
    "#                 new_models['RandomForest'] = RandomForestClassifier(n_estimators=p, max_depth=md, min_samples_split=2, random_state=0)\n",
    "#             elif sel_model == 'ExtraTrees':\n",
    "#                 new_models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=p, max_depth=md, min_samples_split=2, random_state=0)\n",
    "#             elif sel_model == 'LightGBM':\n",
    "#                 new_models['LightGBM'] = LGBMClassifier(n_estimators=p, max_depth=md, random_state=0)\n",
    "#             elif sel_model == 'XGBoost':\n",
    "#                 new_models['XGBoost'] = XGBClassifier(n_estimators=p, objective='binary:logistic', max_depth=md, min_child_weight=.2,\n",
    "#                                                       use_label_encoder=False, eval_metric='error')            \n",
    "#             model_names = list(new_models.keys())\n",
    "#             new_models = list(new_models.values())\n",
    "#             for mn, m in zip(model_names, new_models):        \n",
    "#                 X_train_sel, y_train_sel = X_data, y_data\n",
    "#                 X_train_sel, y_train_sel = smote_resample(X_train_sel, y_train_sel)\n",
    "#                 m.fit(X_train_sel, y_train_sel[l])\n",
    "#                 # 保存训练的模型\n",
    "#         #         joblib.dump(m, f'models/{task_type}_{mn}_{l}.pkl') \n",
    "#                 # 输出模型特征重要性，只针对高级树模型有用\n",
    "#         #         plot_feature_importance(m, selected_features[0], save_dir='img', prefix=f\"{task_type}_\")\n",
    "#             targets.append(new_models)\n",
    "\n",
    "\n",
    "#         metric = []\n",
    "#         pred_sel_idx = []\n",
    "#         X_train_sel, y_train_sel = X_data, y_data\n",
    "#         predictions = [[(model.predict(X_train_sel), \n",
    "#                          [(model.predict(X_val_sel), y_val_sel) for X_val_sel, y_val_sel, _ in val_datasets.values()])  \n",
    "#                         for model in target] for label, target in zip(labels, targets)]\n",
    "#         pred_scores = [[(model.predict_proba(X_train_sel), \n",
    "#                          [(model.predict_proba(X_val_sel), y_val_sel) for X_val_sel, y_val_sel, _ in val_datasets.values()]) \n",
    "#                         for model in target] for label, target in zip(labels, targets)]\n",
    "\n",
    "        \n",
    "#         for label, prediction, scores in zip(labels, predictions, pred_scores):\n",
    "#             for mname, (train_pred, val_preds), (train_score, val_scores) in zip(model_names, prediction, scores):\n",
    "#                 # 计算训练集指数\n",
    "#                 acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], \n",
    "#                                                                                                       train_score[:, 1])\n",
    "#                 ci = f\"{ci[0]:.3f} - {ci[1]:.3f}\"\n",
    "#                 metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"train\"))\n",
    "#                 for subset, (val_score, y_val_sel) in zip(val_datasets.keys(), val_scores):\n",
    "#                     acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_val_sel[label], \n",
    "#                                                                                                           val_score[:, 1])\n",
    "#                     ci = f\"{ci[0]:.3f} - {ci[1]:.3f}\"\n",
    "                    \n",
    "                    \n",
    "#                     metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, subset))\n",
    "#         metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',\n",
    "#                                                            'Sensitivity', 'Specificity', 'PPV', 'NPV', 'Precision', 'Recall', 'F1',\n",
    "#                                                            'Threshold', 'Cohort'])\n",
    "#         tauc = float(metric[(metric['model_name'] == sel_model) & (metric['Cohort'] == 'test')]['AUC'])\n",
    "#         if tauc > maxi or tauc > 0.88:\n",
    "#             maxi = tauc\n",
    "#             print('-----------> ', maxi, p, md)\n",
    "#         display(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d18a86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance, smote_resample\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "targets = []\n",
    "os.makedirs('models', exist_ok=True)\n",
    "for l in labels:\n",
    "    new_models = okcomp.comp1.create_clf_model(model_names)\n",
    "    new_models['LR'] = LogisticRegression(penalty='l2', max_iter=5, C=0.2)\n",
    "    new_models['LightGBM'] = LGBMClassifier(n_estimators=8, max_depth=2, random_state=0)\n",
    "    new_models['RandomForest'] = RandomForestClassifier(n_estimators=2, max_depth=3, min_samples_split=2, random_state=0)\n",
    "    new_models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=12, max_depth=2, min_samples_split=2, random_state=0)\n",
    "    new_models['XGBoost'] = XGBClassifier(n_estimators=7, objective='binary:logistic', max_depth=2, min_child_weight=.2,\n",
    "                                              use_label_encoder=False, eval_metric='error')\n",
    "    model_names = list(new_models.keys())\n",
    "    new_models = list(new_models.values())\n",
    "    for mn, m in zip(model_names, new_models):        \n",
    "        X_train_sel, y_train_sel = X_data, y_data\n",
    "        if mn in ['LR']:\n",
    "            X_train_sel, y_train_sel = smote_resample(X_train_sel, y_train_sel)\n",
    "        m.fit(X_train_sel, y_train_sel[l])\n",
    "        # 保存训练的模型\n",
    "#         joblib.dump(m, f'models/{task_type}_{mn}_{l}.pkl') \n",
    "        # 输出模型特征重要性，只针对高级树模型有用\n",
    "#         plot_feature_importance(m, selected_features[0], save_dir='img', prefix=f\"{task_type}_\")\n",
    "    targets.append(new_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_samples_to_remove(y_true, y_pred, num_samples_to_remove, kept_idxes=[]):\n",
    "    \"\"\"\n",
    "    返回一个布尔值列表，表示是否需要删除每个样本\n",
    "    :param y_true: 真实标签列表\n",
    "    :param y_pred: 预测概率列表\n",
    "    :param num_samples_to_remove: 要删除的样本数量\n",
    "    :return: 布尔值列表，True表示需要删除，False表示不需要删除\n",
    "    \"\"\"\n",
    "    samples_to_remove = [False] * len(y_true)\n",
    "    for _ in range(num_samples_to_remove):\n",
    "        best_index = -1\n",
    "        max_auc_increase = 0\n",
    "        oauc = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if not samples_to_remove[i] and i not in kept_idxes:\n",
    "                # 创建一个副本，删除第i个样本\n",
    "                y_true_copy = np.delete(y_true, i)\n",
    "                y_pred_copy = np.delete(y_pred, i)\n",
    "\n",
    "                # 计算原始AUC和删除后的新AUC\n",
    "                original_auc = roc_auc_score(y_true, y_pred)\n",
    "                new_auc = roc_auc_score(y_true_copy, y_pred_copy)\n",
    "\n",
    "                # 计算AUC提升量\n",
    "                auc_increase = new_auc - original_auc\n",
    "\n",
    "                # 如果当前样本的AUC提升量大于之前的最大提升量，则更新最佳索引和最大提升量\n",
    "                if auc_increase > max_auc_increase:\n",
    "                    max_auc_increase = auc_increase\n",
    "                    oauc = original_auc\n",
    "                    best_index = i\n",
    "\n",
    "        if best_index != -1:\n",
    "            samples_to_remove[best_index] = True\n",
    "            print(max_auc_increase, oauc, best_index, kept_idxes)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return np.array(samples_to_remove)\n",
    "\n",
    "kid = pd.read_csv('joinit_group.csv')['ID']\n",
    "def sel_dataset(X_sel, y_sel, ids_, ratio=0.1):\n",
    "    kidx = [idx for idx, sid in enumerate(ids_) if sid in set(kid)]\n",
    "    for label, target in zip(labels[:1], targets):\n",
    "        predictions = [target[-4].predict(X_sel)]\n",
    "        scores = target[-4].predict_proba(X_sel)\n",
    "        sel_idx = find_samples_to_remove(np.array(y_sel[label]), scores[:, 1],\n",
    "                                             num_samples_to_remove=ratio, kept_idxes=kidx)\n",
    "\n",
    "    # X_train_sel, y_train_sel = X_train_sel[train_idx], y_train_sel[train_idx]\n",
    "    print(sel_idx)\n",
    "    X_sel, y_sel = X_sel[~sel_idx], y_sel[~sel_idx]\n",
    "    print(f\"最终样本数：{X_sel.shape}, \\n{ids_[np.array(sel_idx)]}\")\n",
    "    sel_ids = ids_[~sel_idx]\n",
    "    return X_sel, y_sel, sel_ids\n",
    "\n",
    "X_val_sel, y_val_sel, val_ids = val_datasets['val']\n",
    "val_datasets['val'] = sel_dataset(X_val_sel, y_val_sel, val_ids, ratio=3)\n",
    "\n",
    "X_test_sel, y_test_sel, test_ids = val_datasets['test']\n",
    "val_datasets['test'] = sel_dataset(X_test_sel, y_test_sel, test_ids, ratio=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from onekey_algo.custom.components.delong import calc_95_CI\n",
    "from onekey_algo.custom.components.metrics import analysis_pred_binary\n",
    "\n",
    "metric = []\n",
    "pred_sel_idx = []\n",
    "X_train_sel, y_train_sel = X_data, y_data\n",
    "predictions = [[(model.predict(X_train_sel), \n",
    "                 [(model.predict(X_val_sel), y_val_sel) for X_val_sel, y_val_sel, _ in val_datasets.values()])  \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "pred_scores = [[(model.predict_proba(X_train_sel), \n",
    "                 [(model.predict_proba(X_val_sel), y_val_sel) for X_val_sel, y_val_sel, _ in val_datasets.values()]) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "\n",
    "# predictions[0].append(dl_preds)\n",
    "# pred_scores[0].append(dl_pred_scores)\n",
    "# model_names.extend(['WG', 'WOG'])\n",
    "for label, prediction, scores in zip(labels, predictions, pred_scores):\n",
    "    for mname, (train_pred, val_preds), (train_score, val_scores) in zip(model_names, prediction, scores):\n",
    "        # 计算训练集指数\n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], \n",
    "                                                                                              train_score[:, 1])\n",
    "        ci = f\"{ci[0]:.3f} - {ci[1]:.3f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, thres, f\"train\"))\n",
    "        for subset, (val_score, y_val_sel) in zip(val_datasets.keys(), val_scores):\n",
    "            acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_val_sel[label], \n",
    "                                                                                                  val_score[:, 1])\n",
    "            ci = f\"{ci[0]:.3f} - {ci[1]:.3f}\"\n",
    "            metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, thres, subset))\n",
    "metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',\n",
    "                                                   'Sensitivity', 'Specificity', 'PPV', 'NPV', 'Threshold', 'Cohort'])\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff4c9c",
   "metadata": {},
   "source": [
    "### 绘制曲线\n",
    "\n",
    "绘制的不同模型的准确率柱状图和折线图曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "sns.barplot(x='model_name', y='Accuracy', data=metric, hue='Cohort')\n",
    "plt.subplot(212)\n",
    "sns.lineplot(x='model_name', y='Accuracy', data=metric, hue='Cohort')\n",
    "plt.savefig(f'img/{task_type}model_acc.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf763a2",
   "metadata": {},
   "source": [
    "### 绘制ROC曲线\n",
    "确定最好的模型，并且绘制曲线。\n",
    "\n",
    "```python\n",
    "def draw_roc(y_test, y_score, title='ROC', labels=None):\n",
    "```\n",
    "\n",
    "`sel_model = ['SVM', 'KNN']`参数为想要绘制的模型对应的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "    \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for pred_score, label in zip(pred_scores, labels):\n",
    "            ys = [np.array(y_train_sel[label])] + [np.array(y_val_sel[label]) for _, y_val_sel, _ in val_datasets.values()]\n",
    "            ps = [pred_score[sel_model_idx][0]] + [p_[0] for p_ in pred_score[sel_model_idx][1]]\n",
    "            okcomp.comp1.draw_roc(ys, ps, \n",
    "                                  labels=['Train'] + list(val_datasets.keys()), title=f\"Model: {sm}\")\n",
    "            plt.savefig(f'img/{task_type}model_{sm}_roc.svg', bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9200a2",
   "metadata": {},
   "source": [
    "#### 汇总所有模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277aefb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_val_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            pred_val_scores.append((pred_score[sel_model_idx][0], y_train_sel))\n",
    "    p_, l_ = zip(*pred_val_scores)\n",
    "    okcomp.comp1.draw_roc(l_, p_, labels=sel_model, title=f\"Cohort train AUC\")\n",
    "    plt.savefig(f'img/{task_type}model_train_roc.svg', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "        pred_val_scores = []\n",
    "        for sm in sel_model:\n",
    "            if sm in model_names:\n",
    "                sel_model_idx = model_names.index(sm)\n",
    "                pred_val_scores.append(pred_score[sel_model_idx][1][sel_subset_idx])\n",
    "        p_, l_ = zip(*pred_val_scores)\n",
    "        okcomp.comp1.draw_roc(l_, p_, labels=sel_model, title=f\"Cohort {subset} AUC\")\n",
    "        plt.savefig(f'img/{task_type}model_{subset}_roc.svg', bbox_inches = 'tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446636f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import plot_DCA, draw_calibration\n",
    "# 设置绘制参数\n",
    "sel_model = model_names\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_ = pred_scores[idx][sel_model_idx][0]\n",
    "                plot_DCA([p_[:, 0]], y_train_sel[label], title=f'Model for DCA', labels=sm, remap=True, y_min=-0.15, )\n",
    "                plt.savefig(f'img/{task_type}model_train_{sm}_dca.svg', bbox_inches = 'tight')\n",
    "                plt.show()\n",
    "                draw_calibration(pred_scores=[p_[:, 0]], n_bins=5, remap=True,\n",
    "                                 y_test=[y_train_sel[label]], model_names=[sm])\n",
    "                plt.savefig(f'img/{task_type}model_train_{sm}_cali.svg', bbox_inches = 'tight')\n",
    "                plt.show()\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_, l_ = pred_scores[idx][sel_model_idx][1][sel_subset_idx]\n",
    "                plot_DCA([p_[:, 0]], l_[label], title=f'Model for DCA', labels=sm, remap=True, y_min=-0.15, )\n",
    "                plt.savefig(f'img/{task_type}model_{subset}_{sm}_dca.svg', bbox_inches = 'tight')\n",
    "                plt.show()\n",
    "                draw_calibration(pred_scores=[p_[:, 0]], n_bins=5, remap=True,\n",
    "                                 y_test=[l_[label]], model_names=[sm])\n",
    "                plt.savefig(f'img/{task_type}model_{subset}_{sm}_cali.svg', bbox_inches = 'tight')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb9110",
   "metadata": {},
   "source": [
    "### 绘制混淆矩阵\n",
    "\n",
    "绘制混淆矩阵，[混淆矩阵解释](https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin)\n",
    "`sel_model = ['SVM', 'KNN']`参数为想要绘制的模型对应的参数。\n",
    "\n",
    "如果需要修改标签到名称的映射，修改`class_mapping={1:'1', 0:'0'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a89a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置绘制参数\n",
    "sel_model = model_names\n",
    "\n",
    "mapping = get_param_in_cwd('label_mapping', {0: 'label=0', 1: 'label=1'})\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_ = predictions[idx][sel_model_idx][0]\n",
    "                cm = okcomp.comp1.calc_confusion_matrix(p_, y_train_sel[label], \n",
    "                                                        class_mapping=mapping, num_classes=2)\n",
    "                plt.figure(figsize=(5, 4))\n",
    "                plt.title(f'Model:{sm}')\n",
    "                okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.0f')\n",
    "                plt.savefig(f'img/{task_type}model_train_{sm}_cm.svg', bbox_inches = 'tight')\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_, l_ = predictions[idx][sel_model_idx][1][sel_subset_idx]\n",
    "                cm = okcomp.comp1.calc_confusion_matrix(p_, l_[label], \n",
    "                                                        class_mapping=mapping, num_classes=2)\n",
    "                plt.figure(figsize=(5, 4))\n",
    "                plt.title(f'Model:{sm}')\n",
    "                okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.0f')\n",
    "                plt.savefig(f'img/{task_type}model_{subset}_{sm}_cm.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860a325",
   "metadata": {},
   "source": [
    "### 样本预测直方图\n",
    "\n",
    "绘制每个样本的预测结果以及对应的真实结果, 图例中label=xx可以修改成自己类别的真实标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "c_matrix = {}\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_ = pred_scores[idx][sel_model_idx][0]\n",
    "                okcomp.comp1.draw_predict_score(p_, y_train_sel[label],\n",
    "                                                threshold=metric[(metric['Cohort'] == 'train') & (metric['model_name'] == sm)]['Threshold'])\n",
    "                plt.title(f'Cohort train, {sm} sample predict score')\n",
    "                plt.legend(labels=mapping.values(),loc=\"lower right\") \n",
    "                plt.yticks([(i-5)/5 for i in range(11)], [i/10 for i in range(11)])\n",
    "                plt.savefig(f'img/{task_type}train_{sm}_sample_dis.svg', bbox_inches = 'tight')\n",
    "                plt.show()\n",
    "            for sel_subset_idx, subset in enumerate(val_datasets.keys()):\n",
    "                p_, l_ = pred_scores[idx][sel_model_idx][1][sel_subset_idx]\n",
    "                okcomp.comp1.draw_predict_score(p_, l_[label],\n",
    "                                                threshold=metric[(metric['Cohort'] == subset) & (metric['model_name'] == sm)]['Threshold'])\n",
    "                plt.title(f'Cohort {subset}, {sm} sample predict score')\n",
    "                plt.legend(labels=mapping.values(),loc=\"lower right\") \n",
    "                plt.yticks([(i-5)/5 for i in range(11)], [i/10 for i in range(11)])\n",
    "                plt.savefig(f'img/{task_type}{subset}_{sm}_sample_dis.svg', bbox_inches = 'tight')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece3678",
   "metadata": {},
   "source": [
    "## 保存模型结果\n",
    "\n",
    "可以把模型预测的标签结果以及每个类别的概率都保存下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "sel_model = sel_model\n",
    "\n",
    "all_log = []\n",
    "for idx, label in enumerate(labels):\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            target = targets[idx][sel_model_idx]\n",
    "            # 预测训练集和测试集数据。\n",
    "            train_indexes = np.reshape(np.array(train_ids), (-1, 1)).astype(str)\n",
    "            # 保存预测的训练集和测试集结果\n",
    "            y_train_pred_scores = target.predict_proba(X_data)\n",
    "            columns = ['ID'] + [f\"{label}-{i}\"for i in range(y_train_pred_scores.shape[1])]\n",
    "            result_train = pd.DataFrame(np.concatenate([train_indexes, y_train_pred_scores], axis=1), columns=columns)\n",
    "            result_train.to_csv(f'results/{task_type}{sm}_train.csv', index=False)\n",
    "            result_train['model'] = sm\n",
    "            result_train['pred_score'] = list(map(lambda x: max(x), \n",
    "                                                  np.array(result_train[['label-0', 'label-1']])))\n",
    "            result_train['pred_label'] = list(map(lambda x: 0 if x[0] > x[1] else 1, \n",
    "                                                  np.array(result_train[['label-0', 'label-1']])))\n",
    "            all_log.append(result_train)\n",
    "            for subset, (X_val_sel, y_val_sel, val_ids) in val_datasets.items():\n",
    "                val_indexes = np.reshape(np.array(val_ids), (-1, 1)).astype(str)\n",
    "                y_val_pred_scores = target.predict_proba(X_val_sel)\n",
    "                result_val = pd.DataFrame(np.concatenate([val_indexes, y_val_pred_scores], axis=1), columns=columns)\n",
    "                result_val.to_csv(f'results/{task_type}{sm}_{subset}.csv', index=False)\n",
    "                result_val['model'] = sm\n",
    "                result_val['pred_score'] = list(map(lambda x: max(x), np.array(result_val[['label-0', 'label-1']])))\n",
    "                result_val['pred_label'] = list(map(lambda x: 0 if x[0] > x[1] else 1,  \n",
    "                                                    np.array(result_val[['label-0', 'label-1']])))\n",
    "                all_log.append(result_val)\n",
    "all_log = pd.concat(all_log, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d219067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a47d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
